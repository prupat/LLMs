{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNMQ56/VchfbK+1O2uUasvG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prupat/LLMs/blob/main/Fine_Tuning_Class_Project_Part_2_PB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TRANSLATION BOT ENGLISH-FRENCH PART 2**"
      ],
      "metadata": {
        "id": "wB38-Kf0wLGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68MlbtlJK7yO",
        "outputId": "db55973f-ec1b-449f-89c0-a9035001e909"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **EVALUATION**"
      ],
      "metadata": {
        "id": "n5zydx67qvvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's load the libraries\n",
        "import torch\n",
        "from transformers import MarianTokenizer, MarianMTModel"
      ],
      "metadata": {
        "id": "D2B_79ZCrc_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentencepiece sacremoses"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFTqbFgGsBGV",
        "outputId": "ac754ef9-b294-4183-8d8f-4c3b076b67bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2023.6.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.3.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's load tokenizer and model\n",
        "model_path = \"/content/drive/MyDrive/Colab Notebooks/fine_tuned_model\"\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# Load model\n",
        "model = MarianMTModel.from_pretrained(model_path)\n"
      ],
      "metadata": {
        "id": "plWFJpHzrpvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "4_ENKef-uKry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's generate predictions\n",
        "def generate_translation(input_text, model, tokenizer, max_length=50):\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "    input_ids = input_ids.to(device)\n",
        "\n",
        "    translation_ids = model.generate(input_ids, max_length=max_length)\n",
        "    translation = tokenizer.decode(translation_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return translation"
      ],
      "metadata": {
        "id": "o-O6aIPUt2oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentence = \"it's cold today with a temperature of 40 degree F\"\n",
        "translated_output = generate_translation(input_sentence, model, tokenizer)\n",
        "\n",
        "# Let's define ANSI escape codes for bold text\n",
        "bold_start = \"\\033[1m\"\n",
        "bold_end = \"\\033[0m\"\n",
        "\n",
        "print(f\"{bold_start}French Translation:{bold_end} {translated_output}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUwk_1-rudp5",
        "outputId": "95929632-8644-4efb-f3d2-568348ed2c9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mFrench Translation:\u001b[0m Il fait froid aujourd'hui avec une température de quarante degrés F.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ANALYSIS**"
      ],
      "metadata": {
        "id": "dntQKj4AnRB0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Pauline Sanchez dataset is comprised of about 176,000 records. For a translation model, this a pretty small dataset. Nevertheless, the translation did produce a good result for basic queries. The biggest challenge is with complex queries, especially when involving lengthy sentences to be translated, the translation is often lost or jagged up. This model works best with basic queries.\n",
        "\n",
        "# **Social and Ethical Implications:**\n",
        "\n",
        "# **Accessibility and Limitations:**\n",
        "\n",
        "***Positive Aspect***: The use of machine translation, even on a relatively small dataset like the Pauline Sanchez dataset, enhances accessibility to basic information for users who speak different languages.\n",
        "\n",
        "***Risks and Challenges***: The limitations observed with complex queries and lengthy sentences highlight potential accessibility issues for users seeking in-depth or detailed information.\n",
        "\n",
        "# **User Expectations**:\n",
        "\n",
        "***Positive Aspect***: Users with basic queries may find the translation results satisfactory, meeting their immediate needs.\n",
        "\n",
        "***Risks and Challenges***: Users with complex queries may experience frustration or inaccuracies, leading to potential disappointment in the performance of the translation model.\n",
        "\n",
        "\n",
        "\n",
        " The challenges with complex queries might resemble issues seen in earlier versions, emphasizing the need for more extensive datasets for continued improvements in handling linguistic complexity. Continuous refinement and consideration of user needs will contribute to mitigating risks and improving the overall performance of translation models.\n"
      ],
      "metadata": {
        "id": "s8G2-1f3iuVl"
      }
    }
  ]
}